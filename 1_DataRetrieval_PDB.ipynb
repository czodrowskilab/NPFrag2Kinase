{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273b199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General python libraries and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import redo\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML\n",
    "import pprint\n",
    "import os\n",
    "import re\n",
    "import warnings # As suggested by https://github.com/mwaskom/seaborn/issues/3486\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import datetime\n",
    "today = datetime.datetime.now()\n",
    "date = f'{today.year}{today.month}{today.day}'\n",
    "pd.set_option('display.max_columns', 75)\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "# Processing chemical data\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, PandasTools, Descriptors, rdMolDescriptors\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "\n",
    "# Accessing Databases\n",
    "import requests\n",
    "import requests_cache\n",
    "import biotite.database.rcsb as rcsb\n",
    "# from pypdb.clients.data.data_types import DataFetcher, DataType\n",
    "# from bs4 import BeautifulSoup \n",
    "from urllib.request import urlretrieve \n",
    "\n",
    "# Data Visualization\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\",font_scale=2)\n",
    "import matplotlib.collections as clt\n",
    "import ptitprince as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa120e",
   "metadata": {},
   "source": [
    "# Exploiting the PDB Search API\n",
    "Code inspired by https://github.com/volkamerlab/teachopencadd/blob/master/teachopencadd/talktorials/T008_query_Data/PDB/talktorial.ipynb\n",
    "and https://www.biotite-python.org/apidoc/biotite.database.rcsb.FieldQuery.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing a cache for the requests library, that caches the HTTP requests made using requests \n",
    "# Cache name is 'rcsb_pdb'; the cache will be stored in memory\n",
    "requests_cache.install_cache(\"rcsb_pdb\", backend=\"memory\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a28080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pdb_ids_by_ec_number_nochimera(ec_number, pfam):\n",
    "       \n",
    "    \"\"\"\n",
    "    Retrieve all PDB identifiers matching a query request based on the EC Number, exclude chimeric proteins.\n",
    "    \n",
    "    Parameters: \n",
    "        ec_number (str): The EC number that is to be queried.\n",
    "        pfam(str):       The  Protein Family (pfam) name  that is to be queried.\n",
    "    \n",
    "    Returns: \n",
    "        (list):          PDB identifiers matching a query request.         \n",
    "    \"\"\"\n",
    "    \n",
    "    # See https://search.rcsb.org/structure-search-attributes.html for a list of all structure search attributes\n",
    "\n",
    "    # Create a query object for the EC number, using the rcsb.FieldQuery function\n",
    "    query_EC = rcsb.FieldQuery(\"rcsb_polymer_entity.rcsb_ec_lineage.id\", exact_match=ec_number)\n",
    "    \n",
    "    # Create a query object for the pfam id, using the rcsb.FieldQuery function\n",
    "    query_pfam = rcsb.FieldQuery(\"rcsb_polymer_entity_annotation.name\", exact_match=pfam)\n",
    "\n",
    "    # Create a query object for non-chimeric proteins only, using the rcsb.FieldQuery function\n",
    "    # Chimeras posses a Polymer Entity Source Count greater 1. This is negated by using the ~\n",
    "    query_nochimera = ~rcsb.FieldQuery(\"rcsb_polymer_entity.rcsb_source_part_count\", greater=1)\n",
    "    \n",
    "    # Combine the queries with the 'and' operator to match only PDB IDs that fulfill all three conditions.\n",
    "    query = rcsb.CompositeQuery([query_EC, query_pfam, query_nochimera], \"and\")\n",
    "    \n",
    "    # Print the count of PDB entries matching the defined EC number as of today\n",
    "    print(f\"{rcsb.count(query)} structures (excluding chimeras) with the EC Number {ec_number}, belong to the protein family {pfam}, are deposited in the PDB as of {date}. \\n\")\n",
    "\n",
    "    # Use the rcsb.search function to retrieve all PDB identifiers matching the query request and return the list\n",
    "    pdb_ids = rcsb.search(query)\n",
    "    return pdb_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the fetch_pdb_ids_by_ec_number function to retrieve all PDB identifiers for the defined EC number and Pfam Name\n",
    "pdb_ids = fetch_pdb_ids_by_ec_number_nochimera(\"2.7.11.11\", \"Protein kinase domain (Pkinase)\") \n",
    "\n",
    "print(*pdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acc594",
   "metadata": {},
   "source": [
    "# Exploiting the PDB Data API to retrieve .pdb metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6f96e",
   "metadata": {},
   "source": [
    "We use a GraphQL-based approach to retrieve the metadata associated with the .pdb structures. <br>\n",
    "Note that we only fetch meta information on PDB structures here, we do not fetch the structures (3D coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ff5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pdb_info(pdb_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieve further information about a PDB structure based on its PDB ID, exploiting the PDB-API.\n",
    "    \n",
    "    Parameters: \n",
    "        pdb_id (str): The PDB ID of the structure to be queried.\n",
    "\n",
    "    Returns: \n",
    "        (dict):       A dictionary containing information about the PDB structure.         \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the GraphQL query, requesting various information about the PDB structure, as a multiline string.\n",
    "    query = f\"\"\" {{ entry(entry_id: \"{pdb_id}\") {{ \n",
    "    rcsb_id \n",
    "    struct {{title}}\n",
    "    exptl {{ method }} \n",
    "    exptl_crystal {{ density_percent_sol }} \n",
    "    exptl_crystal_grow {{ method pH pdbx_details }} \n",
    "    rcsb_accession_info {{ deposit_date }} \n",
    "    rcsb_entry_container_identifiers {{ entry_id }} \n",
    "    rcsb_entry_info {{ deposited_nonpolymer_entity_instance_count deposited_polymer_entity_instance_count \n",
    "    resolution_combined structure_determination_methodology }} \n",
    "    rcsb_primary_citation {{ pdbx_database_id_DOI }} \n",
    "    polymer_entities {{ entity_poly {{ pdbx_seq_one_letter_code_can rcsb_entity_polymer_type rcsb_sample_sequence_length rcsb_mutation_count}} \n",
    "    rcsb_polymer_entity {{ pdbx_number_of_molecules formula_weight pdbx_description pdbx_mutation rcsb_enzyme_class_combined {{ ec }} }}             \n",
    "    rcsb_entity_source_organism {{ ncbi_scientific_name ncbi_taxonomy_id rcsb_gene_name {{ value }} }} }} \n",
    "    assemblies {{ pdbx_struct_assembly {{ oligomeric_count }} }} \n",
    "    refine {{ ls_R_factor_R_free }} \n",
    "    struct_keywords {{ pdbx_keywords }} \n",
    "    }} }}\n",
    "        \"\"\"\n",
    "\n",
    "    # Create the query URL by appending the query to the RCSB PDB API base URL.\n",
    "    query_url = f\"https://data.rcsb.org/graphql?query={query}\"\n",
    "    \n",
    "    # Send the GET request to the RCSB PDB API and retrieve the response.\n",
    "    response = requests.get(query_url)\n",
    "    \n",
    "    # Raise an exception if the request failed.\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse the JSON response and store it in the 'info' variable.\n",
    "    info = response.json()\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271e02b",
   "metadata": {},
   "source": [
    "Each structureâ€™s metadata is returned as a dictionary, i.e. the pdb_infos resembles a list of dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bba6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the PDB IDs in the 'pdb_id' list and call the 'retrieve_pdb_info' function to get further info on each structure \n",
    "# Exploiting the tqdm module to display a progress bar\n",
    "pdb_infos = [retrieve_pdb_info(pdb_id) for pdb_id in tqdm(pdb_ids, desc=\"Retrieve PDB Infos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_none_values(obj):\n",
    "    \"\"\"\n",
    "    Replaces None values in a nested object with 'NaN' strings.\n",
    "\n",
    "    Parameters:\n",
    "        obj (dict or list): The nested object to be processed.\n",
    "\n",
    "    Returns:\n",
    "        None. The input object is modified in-place.\n",
    "    \"\"\"\n",
    "    # If the input object is a dictionary, iterate over every key in the dictionary\n",
    "    if isinstance(obj, dict):\n",
    "        for key in obj:\n",
    "            # If the value corresponding to the current key is None, replace with the 'NaN' string\n",
    "            if obj[key] is None:\n",
    "                obj[key] = 'NaN'\n",
    "            # If the value is a nested dictionary or list, recursively call the function.\n",
    "            elif isinstance(obj[key], (dict, list)):\n",
    "                replace_none_values(obj[key])\n",
    "\n",
    "    # If the input object is a list, iterate over each item in the list and recursively call the function \n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            replace_none_values(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_info_to_file(filename, retrieve_info, pdb_ids):\n",
    "    \"\"\"\n",
    "    Save the received information on each structure to a file.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str):             The name of the file to save the information to.\n",
    "        retrieve_info (function):   A function that retrieves the information for a given PDB ID.\n",
    "        pdb_ids (list):             A list of PDB IDs to retrieve the information for.\n",
    "    \"\"\"\n",
    " \n",
    "    with open(filename, \"w\") as output_file:\n",
    "\n",
    "        # Iterate over every PDB ID in the list \n",
    "        for pdb_id in pdb_ids:\n",
    "\n",
    "            # Retrieve the information for the current PDB ID\n",
    "            pdb_info = retrieve_info(pdb_id)\n",
    "\n",
    "            # Write the PDB ID and its information to the output file\n",
    "            output_file.write(f\"PDB ID: {pdb_id}\\n\")\n",
    "            pprint.pprint(pdb_info, stream=output_file, width=1)\n",
    "\n",
    "            # Add two line breaks between each element of the list\n",
    "            output_file.write(\"\\n\\n\")\n",
    "\n",
    "filepath = os.path.join('Data', 'PDB')\n",
    "tmp_folder = os.path.join(filepath, 'Temp')            \n",
    "save_info_to_file(os.path.join(tmp_folder, 'PDB_API_StructureMetadata.txt'), retrieve_pdb_info, pdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d62318",
   "metadata": {},
   "source": [
    "## Create dataframe\n",
    "We next extract the metadata using the  helper functions 'extract_from_dict' and create a pandas data frame with all the relevant metadata per structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89113e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_dict(dictionary, keyword):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function to extract all the values associated with a keyword from a nested dictionary.\n",
    "        \n",
    "    Parameters: \n",
    "        dictionary (dict): The dictionary from which the values need to be extracted.\n",
    "        keyword (str):     The keyword associated with the values that need to be extracted.\n",
    "    \n",
    "    Returns: \n",
    "        (list):            The extracted values from the dictionary. \n",
    "                           If the list comprises only one element, the element itself is returned, instead of a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list \n",
    "    results = [] \n",
    "\n",
    "    def recursive_search(dictionary, keyword):\n",
    "        \n",
    "        \"\"\"\n",
    "        A recursive helper function that searches for the keyword in the nested dictionary \n",
    "        and adds its associated values to the results list.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if the current item is a dictionary\n",
    "        if isinstance(dictionary, dict):\n",
    "            \n",
    "            # Check if the keyword is found in the dictionary\n",
    "            if keyword in dictionary:\n",
    "                \n",
    "                # Add the value associated with the keyword to the results list\n",
    "                results.append(dictionary[keyword])\n",
    "                \n",
    "            # Iterate over each key-value pair in the dictionary    \n",
    "            for key, value in dictionary.items():\n",
    "                \n",
    "                # If the value is a dictionary, recursively search for the keyword in the nested dictionary\n",
    "                if isinstance(value, dict):\n",
    "                    recursive_search(value, keyword)\n",
    "                \n",
    "                # If the value is a list, iterate over each item in the list and \n",
    "                # recursively search for the keyword in the nested item\n",
    "                elif isinstance(value, list):\n",
    "                    for item in value:\n",
    "                        recursive_search(item, keyword)\n",
    "\n",
    "    recursive_search(dictionary, keyword)\n",
    "        \n",
    "    if len(results) == 0: \n",
    "        results = ['NaN']\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c917e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdb_info_df(pdb_ids, pdb_infos):\n",
    "    \"\"\"\n",
    "    Create a data frame containing PDB (Protein Data Bank) information.\n",
    "\n",
    "    Parameters:\n",
    "        pdb_ids   (List[str]):  List of PDB IDs.\n",
    "        pdb_infos (List[dict]): List of dictionaries containing PDB information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame containing PDB information.\n",
    "\n",
    "    \"\"\"\n",
    "    info = [{\n",
    "            'PDB_ID':                         extract_from_dict(pdb_info, \"rcsb_id\")[0],\n",
    "            'Title':                          extract_from_dict(pdb_info, \"title\")[0], \n",
    "            'Nonpolymer_Entity_Count':        extract_from_dict(pdb_info, \"deposited_nonpolymer_entity_instance_count\")[0],\n",
    "            'Protein_Entity_Count':           extract_from_dict(pdb_info, \"deposited_polymer_entity_instance_count\")[0],\n",
    "            'Exp_Method':                     extract_from_dict(pdb_info, 'exptl'),\n",
    "            'Cryst_Method':                   extract_from_dict(pdb_info, \"exptl_crystal_grow\")[0], \n",
    "            #'pH':                            extract_from_dict(pdb_info, \"pH\")[0],\n",
    "            'Deposit_Date':                   extract_from_dict(pdb_info, \"deposit_date\")[0],\n",
    "            'Resolution':                     extract_from_dict(pdb_info, \"resolution_combined\")[0],\n",
    "            'DOI':                            extract_from_dict(pdb_info, \"pdbx_database_id_DOI\")[0],\n",
    "            'Polymer_Entity_Dict':            extract_from_dict(pdb_info, \"entity_poly\"),\n",
    "            'Source_Organism':                extract_from_dict(pdb_info, \"rcsb_entity_source_organism\"),\n",
    "            'Polymer_Entity':                 extract_from_dict(pdb_info, \"rcsb_polymer_entity\"),\n",
    "            'R_Free':                         extract_from_dict(pdb_info, \"ls_R_factor_R_free\"), \n",
    "            'Keywords':                       extract_from_dict(pdb_info, \"pdbx_keywords\"),\n",
    "        } for pdb_info in pdb_infos]\n",
    "\n",
    "    pdb_info_df = pd.DataFrame(info)\n",
    "\n",
    "    return pdb_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the create_pdb_info_df function to create a pandas.data frame containing the PDB information\n",
    "pdb_info_df = create_pdb_info_df(pdb_ids, pdb_infos)\n",
    "pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb95c0",
   "metadata": {},
   "source": [
    "# Parse .pdb Files to retrieve further info\n",
    "\n",
    "https://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html#REMARK%20280 <br>\n",
    "**REMARK280** presents information about the crystal. [...] Crystallization conditions are in free text.\n",
    "\n",
    "https://www.wwpdb.org/documentation/file-format-content/format32/remarks2.html#REMARK%20610 <br>\n",
    "**REMARK 610** enumerates non-polymer residues with missing atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163dcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_remarks_from_pdb_file(pdb_ids, remark):\n",
    "    \"\"\"\n",
    "    Extracts remark information from PDB files for a given list of PDB IDs.\n",
    "    Parameters:\n",
    "        pdb_ids (list):  List of PDB IDs to extract remarks for\n",
    "        remark (str):    Type of remark to extract (e.g. 'REMARK 280', 'REMARK 610')\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the extracted remark information\n",
    "    \"\"\"\n",
    "     # Initialize an empty list to store the extracted data\n",
    "    data = [] \n",
    "    # Iterate over the list of pdb_ids\n",
    "    for pdb_id in pdb_ids:\n",
    "        url = f'https://files.rcsb.org/view/{pdb_id}.pdb'  \n",
    "        # Send a GET request to read in the PDB file (without saving it locally)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:  # Check if the request was successful\n",
    "            # Extract the remark lines from the PDB file, removing the remark type and any leading/trailing whitespace\n",
    "            remark_lines = [line[len(remark):] for line in response.text.splitlines() if line.startswith(remark)]\n",
    "            # Create a dictionary with the PDB ID and the extracted remark information, and add it to the data list\n",
    "            data.append({'PDB_ID': pdb_id,  remark.replace(' ', '_'): re.sub(' +', ' ', ' '.join(remark_lines))})\n",
    "        else:\n",
    "            # If the request failed, add an error message to the data list\n",
    "            data.append({'PDB_ID': pdb_id,  remark.replace(' ', '_'): f'Failed to retrieve {pdb_id}.pdb. Status code: {response.status_code}'})\n",
    "    # Convert the data list to a Pandas DataFrame and return the latter\n",
    "    return pd.DataFrame(data)  \n",
    "\n",
    "# Merge the extracted remark information into the pdb_info_df DataFrame\n",
    "pdb_info_df = pdb_info_df.merge(extract_remarks_from_pdb_file(pdb_ids, 'REMARK 280'), on='PDB_ID', how='left')\n",
    "pdb_info_df = pdb_info_df.merge(extract_remarks_from_pdb_file(pdb_ids, 'REMARK 610'), on='PDB_ID', how='left')\n",
    "\n",
    "#pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762ef74",
   "metadata": {},
   "source": [
    "# Post-Processing of the pdb_info_df data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ee4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specified substring from the REMARK_610 column\n",
    "substring = \"MISSING HETEROATOM THE FOLLOWING RESIDUES HAVE MISSING ATOMS (M=MODEL NUMBER; RES=RESIDUE NAME; C=CHAIN IDENTIFIER; SSEQ=SEQUENCE NUMBER; I=INSERTION CODE): M RES C SSEQI\"\n",
    "pdb_info_df['REMARK_610'] = pdb_info_df['REMARK_610'].str.replace(substring, '', regex=False)\n",
    "\n",
    "# Extract numbers following the 'PH' substring from the REMARK_280 column, based on regular expressions\n",
    "# PH: matches the literal 'PH' substring\n",
    "# (?:=| )?: matches an optional '=' or ' ' character (non-capturing group)\n",
    "# (-?\\d+(?:\\.\\d+)?): captures the number (including optional negative sign and decimal part)\n",
    "pdb_info_df['pH'] = pdb_info_df['REMARK_280'].apply(lambda x: re.search(r'PH(?:=| )?(-?\\d+(?:\\.\\d+)?)', x).group(1) if re.search(r'PH(?:=| )?(-?\\d+(?:\\.\\d+)?)', x) else '')\n",
    "\n",
    "# Drop the original columns\n",
    "#pdb_info_df = pdb_info_df.drop(['REMARK_280', 'REMARK_610'], axis=1)\n",
    "\n",
    "pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_from_dict_in_column(df, column_name, search_term):\n",
    "    \"\"\"\n",
    "    Extracts values from a dictionary located in a specified column of a DataFrame that match a search term \n",
    ".\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to extract values from.\n",
    "        column_name (str):     The name of the column to search for dictionaries.\n",
    "        search_term (str):     The term to search for in the dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted values from the dictionaries in the column that match the search term.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the extracted values\n",
    "    new_list = []\n",
    "    \n",
    "    # For each row i in the df, extract the data from the specified column, corresponding to the search term  \n",
    "    for i in range(len(df)):   \n",
    "        data = df.iloc[i][column_name]   \n",
    "        extracted_values = extract_values(data, search_term)\n",
    "        # If extracted data is not None, append to new_list, otherwise add the 'NaN' string\n",
    "        if extracted_values != None: \n",
    "            new_list.append(extracted_values)\n",
    "        else: \n",
    "            new_list.append('NaN')\n",
    "\n",
    "    # If no values were extracted at all, append 'NaN' to the list\n",
    "    if len(new_list) == 0: \n",
    "        new_list.append('NaN')\n",
    "        \n",
    "    return new_list\n",
    "\n",
    "\n",
    "def extract_values(data, search_term):\n",
    "    \"\"\"\n",
    "    Helper function to extract values from a list or dictionary matching a search term.\n",
    "\n",
    "    Parameters:\n",
    "        data (list or dict):  The list or dictionary to extract values from.\n",
    "        search_term (str):    The term to search for in the list or dictionary.\n",
    "\n",
    "    Returns:\n",
    "        list or str:          A list of extracted values from the list or dictionary that match the search term, \n",
    "                              or the string 'NaN' if the input is neither a list nor a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the input data is a list\n",
    "    if isinstance(data, list):\n",
    "        \n",
    "        # Iterate over each element in the list and return a new list containing the extracted values from each sub-dictionary in the list\n",
    "        return [extract_values(subdict, search_term) for subdict in data]\n",
    "    \n",
    "    # If the input data is a dictionary\n",
    "    elif isinstance(data, dict):\n",
    "        \n",
    "        # Extract the values corresponding to the search term\n",
    "        return data.get(search_term, [])\n",
    "    \n",
    "    # If the input data is neither a list nor a dictionary, return the 'NaN' string \n",
    "    else:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the entries in the specified column from a list comprising a single dictionary to the dictionary itself\n",
    "db_info_df = pdb_info_df.explode(['R_Free'], ignore_index=True)\n",
    "\n",
    "# Call the extract_values_from_dict_in_column function to extract the values\n",
    "pdb_info_df['Exp_Method'] = extract_values_from_dict_in_column(pdb_info_df, 'Exp_Method', 'method')\n",
    "\n",
    "# Flatten the list of lists in the Exp_Method column to a simple list \n",
    "pdb_info_df['Exp_Method'] = pdb_info_df['Exp_Method'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "\n",
    "# If there is only one value in the Exp_Method list, extract the value, else return the original list\n",
    "pdb_info_df['Exp_Method'] = [(x[0]) if len(x) == 1 else x for x in pdb_info_df['Exp_Method']]\n",
    "\n",
    "print(pdb_info_df['Exp_Method'].value_counts())\n",
    "\n",
    "# Explode to separate for each data collection method, e.g. in case of joint XRD/ND\n",
    "pdb_info_df = pdb_info_df.explode(['R_Free', 'Exp_Method', 'Resolution'], ignore_index=True)\n",
    "\n",
    "# Drop the rows for all structures that were determined by other methods than X-Ray crystallography\n",
    "pdb_info_df = pdb_info_df[pdb_info_df['Exp_Method'].str.contains('X-RAY DIFFRACTION', case=False)]\n",
    "pdb_info_df = pdb_info_df.reset_index(drop=True)\n",
    "\n",
    "# pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the extract_values_from_dict_in_column function to extract the values into two helper columns\n",
    "pdb_info_df['Cryst_Method_Method']  = extract_values_from_dict_in_column(pdb_info_df, 'Cryst_Method', 'method')\n",
    "pdb_info_df['Cryst_Method_Details'] = extract_values_from_dict_in_column(pdb_info_df, 'Cryst_Method', 'pdbx_details')\n",
    "\n",
    "for col in ['Cryst_Method_Method', 'Cryst_Method_Details']:\n",
    "    # If there is only one value in the list, extract the value, else return the original list\n",
    "    pdb_info_df[col] = [(x[0]) if len(x) == 1 and type(x) is list else x for x in pdb_info_df[col]]\n",
    "\n",
    "    # Iterate through each entry in the column and replace the None and 'None' values with 'NaN'. \n",
    "    # Else, keep the original value.\n",
    "    pdb_info_df[col] = pdb_info_df[col].apply(lambda x: 'NaN' if x in [None, 'NaN', 'None'] or type(x) is list and len(x) == 1 and (x[0] is None or x[0] == 'NaN' or x[0] == 'None') else x)\n",
    "\n",
    "    # Transform the entries to strings\n",
    "    pdb_info_df[col] =  pdb_info_df[col].apply(lambda x: ', '.join(map(str, x)) if type(x) is list else x)\n",
    "\n",
    "# Join the strings from the 'Cryst_Method_Details' and 'Cryst_Method_Method' column together\n",
    "pdb_info_df['Cryst_Method'] = pdb_info_df['Cryst_Method_Method'].str.cat(pdb_info_df['Cryst_Method_Details'], sep='; ')\n",
    "\n",
    "# Drop the helper columns 'Cryst_Method_Method' and 'Cryst_Method_Details' from the df\n",
    "pdb_info_df = pdb_info_df.drop(['Cryst_Method_Method', 'Cryst_Method_Details'], axis=1)\n",
    "\n",
    "pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d291e7d",
   "metadata": {},
   "source": [
    "**Crystallization technique (co-crystallized vs. soaked)** <br>\n",
    "First, search title, Remark 280 and crystallization fields for keywords \"soak\" or \"cocrystal\" and variations. <br>\n",
    "Next, for those structures with no annotation found and a doi present, manually check the original publications and add the info to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soaking_or_cocrystallization(x):\n",
    "    # Check if the string 'soak' is present in the input string\n",
    "    if 'soak' in x.lower():\n",
    "        # If yes, return 'soak'\n",
    "        return 'soaked'\n",
    "    # Check, if string 'cocrystal' or 'co-crystal' is present in the input string\n",
    "    elif any(var in x.lower() for var in ['cocrystal', 'co-crystal']):\n",
    "        # If either 'cocrystal' or 'co-crystal' is present, return 'co-crystallize'\n",
    "        return 'co-crystallized'\n",
    "    # If neither 'soak' nor 'cocrystal'/'co-crystal' is present, return 'NaN'\n",
    "    else:\n",
    "        return 'NaN' \n",
    "\n",
    "# Apply the soaking_or_cocrystallization function to each row of the pdb_info_df DataFrame\n",
    "pdb_info_df['Cryst_Technique'] = pdb_info_df.apply(lambda x: \n",
    "                                                # For each row, apply the soaking_or_cocrystallization function to 3 columns: 'Cryst_Method', 'Title', and 'REMARK_280'\n",
    "                                                next((val for val in [\n",
    "                                                                soaking_or_cocrystallization(x['Cryst_Method']), \n",
    "                                                                soaking_or_cocrystallization(x['Title']), \n",
    "                                                                soaking_or_cocrystallization(x['REMARK_280'])\n",
    "                                                                ] if not pd.isna(val)), 'NaN'), axis=1)\n",
    "\n",
    "# Print the value counts of the 'Cryst_Method' column\n",
    "print(pdb_info_df['Cryst_Technique'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists origin from manually checking the original publications\n",
    "cocrystallized = [ \"1CDK\", \"1CMK\", \"2GNF\", \"2GNH\", \"2GNI\", \"2GNJ\", \"2GNL\", \"2QUR\", \"2UZT\", \"2UZU\", \"2UZV\", \"2UZW\", \"3AG9\", \"3AGL\", \"3BWJ\", \"3FJQ\", \"3L9L\", \"3L9M\", \"3L9N\", \"3MVJ\", \"3QAL\", \"3QAM\", \"3VQH\", \"4C34\", \"4DFX\", \"4DFZ\", \"4DG0\", \"4DG2\", \"4DH1\", \"4DH3\", \"4DH5\", \"4DH7\", \"4DH8\", \"4HPT\", \"4HPU\", \"4IAC\", \"4IAD\", \"4IAF\", \"4IAI\", \"4IAK\", \"4IAY\", \"4IAZ\", \"4IB0\", \"4IB1\", \"4IB3\", \"4O21\", \"4UJ1\", \"4UJ2\", \"4UJ9\", \"4UJA\", \"4UJB\", \"4WB5\", \"4WB6\", \"4WB8\", \"4XW4\", \"4XW5\", \"4XW6\", \"4Z83\", \"4Z84\", \"5IZF\", \"5IZJ\", \"5J5X\", \"5NW8\", \"5O0E\", \"5O5M\", \"5OK3\", \"5OL3\", \"5OT3\", \"6C0U\", \"6E21\", \"6E99\", \"6E9L\", \"6EH2\", \"6EM6\", \"6EM7\", \"6EMA\", \"6ERT\", \"6ERW\", \"6ESA\", \"6I2A\", \"6I2B\", \"6I2C\", \"6I2D\", \"7PID\", \"7PIE\", \"7PIF\", \"7PIG\", \"7PIH\", \"7PNS\", \"7UJX\", \"7V0G\", \"7Y1G\", \"8SF8\" ]\n",
    "soaked = [ \"2JDS\", \"2JDT\", \"2JDV\", \"2OH0\", \"2OJF\", \"2UW3\", \"2UW4\", \"2UW5\", \"2UW8\", \"2VNW\", \"2VNY\", \"2VO0\", \"2VO3\", \"2VO6\", \"2VO7\", \"3AMA\", \"3AMB\", \"3DND\", \"3DNE\", \"3NX8\", \"3X2U\", \"3X2V\", \"3X2W\", \"3ZO1\", \"3ZO2\", \"3ZO3\", \"3ZO4\", \"4AXA\", \"4C35\", \"4C36\", \"4C37\", \"4C38\", \"4IE9\", \"4IJ9\", \"4NTT\", \"4YXR\", \"4YXS\", \"5N33\", \"5N3C\", \"5N3D\", \"5N3E\", \"5N3H\", \"5N3J\", \"5N3Q\", \"5N3S\", \"6EM2\", \"6SNN\", \"6SNX\", \"6SOX\", \"6SPM\", \"6SPS\", \"6SPU\", \"6SPY\", \"6YPS\", \"6ZN0\" ]\n",
    "\n",
    "# Replace NaN values in Cryst_Technique column\n",
    "pdb_info_df.loc[pdb_info_df['PDB_ID'].isin(soaked)        , 'Cryst_Technique'] = 'soaked'\n",
    "pdb_info_df.loc[pdb_info_df['PDB_ID'].isin(cocrystallized), 'Cryst_Technique'] = 'co-crystallized'\n",
    "\n",
    "# Print the value counts of the 'Cryst_Technique' column\n",
    "print(pdb_info_df['Cryst_Technique'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the first 10 characters that correspond to the date (neglecting the time)\n",
    "pdb_info_df['Deposit_Date'] = pdb_info_df['Deposit_Date'].str[:10]\n",
    "\n",
    "# Overwrite the 'DOI' column so that it contains doi links instead of simple strings, except for 'NaN' values\n",
    "pdb_info_df['DOI'] = pdb_info_df['DOI'].apply(lambda x: f'https://doi.org/{x}' if x not in [None, 'NaN', 'None'] else 'NaN')\n",
    "\n",
    "# If there is only one value in the resolution list, extract the value and convert to float, else return the original list\n",
    "pdb_info_df['Resolution'] = [float(x[0]) if isinstance(x, list) and len(x) == 1 else x for x in pdb_info_df['Resolution']]\n",
    "\n",
    "# Display data frame\n",
    "pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the entries in the specified columns from a list comprising a single dictionary to the dictionary itself\n",
    "pdb_info_df = pdb_info_df.explode(['Polymer_Entity_Dict', 'Polymer_Entity', 'Source_Organism'], ignore_index=True)\n",
    "\n",
    "# Another round of flattening is required for the column specified\n",
    "pdb_info_df = pdb_info_df.explode(['Source_Organism'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58832d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the extract_values_from_dict_in_column function to extract the values into individual columns\n",
    "pdb_info_df['Sequence']         = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity_Dict',  'pdbx_seq_one_letter_code_can')\n",
    "pdb_info_df['Sequence_Length']  = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity_Dict',  'rcsb_sample_sequence_length')\n",
    "pdb_info_df['Mutation_Count']   = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity_Dict',  'rcsb_mutation_count')\n",
    "pdb_info_df.drop('Polymer_Entity_Dict', axis=1, inplace=True)\n",
    "\n",
    "pdb_info_df['Gene_Name']        = extract_values_from_dict_in_column(pdb_info_df, 'Source_Organism', 'rcsb_gene_name')\n",
    "pdb_info_df['Source_Organism']  = extract_values_from_dict_in_column(pdb_info_df, 'Source_Organism', 'ncbi_scientific_name')\n",
    "\n",
    "pdb_info_df['MW']               = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity',  'formula_weight')\n",
    "pdb_info_df['Name']             = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity',  'pdbx_description')\n",
    "pdb_info_df['EC']               = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity',  'rcsb_enzyme_class_combined')\n",
    "pdb_info_df['Mutations']        = extract_values_from_dict_in_column(pdb_info_df, 'Polymer_Entity',  'pdbx_mutation')\n",
    "pdb_info_df.drop('Polymer_Entity', axis=1, inplace=True)\n",
    "\n",
    "pdb_info_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the entries in the specified column from a list comprising a single dictionary to the dictionary itself\n",
    "pdb_info_df = pdb_info_df.explode('EC', ignore_index=True)\n",
    "\n",
    "# Call the extract_values_from_dict_in_column function to extract the values into individual columns\n",
    "pdb_info_df['EC_Number'] = extract_values_from_dict_in_column(pdb_info_df, 'EC', 'ec')\n",
    "\n",
    "# Drop the original column\n",
    "pdb_info_df.drop('EC', axis=1, inplace=True)\n",
    "\n",
    "pdb_info_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the entries in the specified column from a list comprising a single dictionary to the dictionary itself\n",
    "pdb_info_df = pdb_info_df.explode('Gene_Name', ignore_index=True)\n",
    "\n",
    "# Call the extract_values_from_dict_in_column function to extract the values into individual columns\n",
    "pdb_info_df['Gene_Name'] = extract_values_from_dict_in_column(pdb_info_df, 'Gene_Name', 'value')\n",
    "\n",
    "# Convert all strings in the Gene_Name column to uppercase strings\n",
    "pdb_info_df['Gene_Name'] = pdb_info_df['Gene_Name'].str.upper()\n",
    "\n",
    "# Replacing 'PRKACA' by 'PKACA' in the 'Gene_Name' column, as these are synonyms\n",
    "pdb_info_df['Gene_Name'] = pdb_info_df['Gene_Name'].str.replace('PRKACA', 'PKACA')\n",
    "# Replacing the duplicate 'PKACA PKACA' by the single 'PKACA' in the 'Gene_Name' column\n",
    "pdb_info_df['Gene_Name'] = pdb_info_df['Gene_Name'].str.replace('PKACA PKACA', 'PKACA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be58949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the entries in the specified column from a list comprising a single dictionary to the dictionary itself\n",
    "pdb_info_df = pdb_info_df.explode('Keywords', ignore_index=True)\n",
    "\n",
    "# String Manipulation: First letter uppercase, all others lower case, for the specified columns\n",
    "# Exceptions are the words 'NaN' and 'pH'\n",
    "cols_string_manipulation = ['Name', 'Cryst_Method', 'REMARK_280', 'Keywords', 'Exp_Method' ]\n",
    "pdb_info_df[cols_string_manipulation] = pdb_info_df[cols_string_manipulation].map(lambda x: x.title() if isinstance(x, str) else x)\n",
    "pdb_info_df[cols_string_manipulation] = pdb_info_df[cols_string_manipulation].map(lambda x: x.replace('Nan', 'NaN').replace('Ph', 'pH').replace('Camp', 'cAMP') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace any empty cells with 'NaN'\n",
    "pdb_info_df = pdb_info_df.replace({'': 'NaN', pd.NA: 'NaN', 'Nan': 'NaN', 'NAN': 'NaN'})\n",
    "\n",
    "# print(len(pdb_info_df))\n",
    "\n",
    "# # Drop rows where all values in the specified columns are NaN (missing values)\n",
    "# pdb_info_df = pdb_info_df.dropna(\n",
    "#     subset=['Sequence', 'Sequence_Length', 'Mutation_Count', 'MW', 'Name', 'EC_Number', 'Gene_Name', 'Source_Organism'], \n",
    "#     how='all')\n",
    "\n",
    "# # Reset index\n",
    "# pdb_info_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(pdb_info_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d05207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any entry in the data frame is a list, convert that list to a string by joining the entries together\n",
    "def string_join_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return ', '.join(map(str, val))\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "# Use the pandas 'map' function to apply the conversion function to every cell in the data frame\n",
    "pdb_info_df = pdb_info_df.map(string_join_list)\n",
    "\n",
    "pdb_info_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d79426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows in the data frame, and keep only the first! \n",
    "pdb_info_df = pdb_info_df.drop_duplicates(keep='first')\n",
    "\n",
    "print(len(pdb_info_df))\n",
    "pdb_info_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e1ad4",
   "metadata": {},
   "source": [
    "### Cross Check: Which EC-Numbers are present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a210491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_numbers_present = pdb_info_df.EC_Number.unique().tolist()\n",
    "print(ec_numbers_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a504cb",
   "metadata": {},
   "source": [
    "'2.7.11.11' resembles Protein Kinase A\n",
    "\n",
    "'2.7.1.37' is for protein kinases in general. \n",
    "Outdated - Dual labelling was only used for structures resolved before 2007!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a27963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the rows, for which no EC-Number is contained \n",
    "# => e.g. for the peptidic inhibitor present in nearly every structure\n",
    "pdb_info_df_no_EC = pdb_info_df[pdb_info_df['EC_Number'] == 'NaN'] \n",
    "pdb_info_df_no_EC = pdb_info_df_no_EC.sort_values('MW', ascending=False)\n",
    "print(len(pdb_info_df_no_EC))\n",
    "#pdb_info_df_no_EC.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63275a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a row in the data frame based on the entry in the PDB_ID column\n",
    "# pdb_info_df[pdb_info_df['PDB_ID'] == '6ESA'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ba68f",
   "metadata": {},
   "source": [
    "## Write beautified data frame to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_info_df.to_excel(os.path.join(tmp_folder, f\"PDB_API_StructureMetadata.xlsx\"), index=False)\n",
    "pdb_info_df.to_csv(os.path.join(tmp_folder, f\"PDB_API_StructureMetadata.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b54987",
   "metadata": {},
   "source": [
    "# Exclude structures, not comprising any non-polymer entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4383dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pdb_info_df))\n",
    "\n",
    "# Exclude structures, not comprising any non-polymer entity\n",
    "pdb_info_df = pdb_info_df[pdb_info_df['Nonpolymer_Entity_Count'] != 0]\n",
    "pdb_info_df = pdb_info_df.reset_index(drop=True)\n",
    "\n",
    "print(len(pdb_info_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b9ab1",
   "metadata": {},
   "source": [
    "# Filtering based on Gene Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0df203",
   "metadata": {},
   "outputs": [],
   "source": [
    "genenames = sorted(list(pdb_info_df.Gene_Name.unique()))\n",
    "\n",
    "print(genenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6e7d0",
   "metadata": {},
   "source": [
    "| Gene Name     | Encoded protein                                                                | Protein Type       |\n",
    "|---------------|--------------------------------------------------------------------------------|--------------------|\n",
    "| PKA           | too generic, check the corresponding structures!                               | (generic)          |\n",
    "| PKACA         | Protein Kinase A, *catalytic* subunit alpha                                    | catalytic subunit! |\n",
    "| PKIA          | cAMP-dependent protein kinase *inhibitor* alpha                                | Peptidic Inhibitor |\n",
    "| PKIA PRKACN1  | cAMP-dependent protein kinase *inhibitor* alpha                                | Peptidic Inhibitor |\n",
    "| PLN           | *Phospholamban*, a calcium transporter protein (1.7 kDa)                       | Substrate of PKA   |\n",
    "| PRKACN1       | cAMP-dependent protein kinase *inhibitor* alpha                                | Peptidic Inhibitor |\n",
    "| PRKAR1A       | cAMP-dependent protein kinase type I-alpha *regulatory* subunit                | Regulatory Subunit |\n",
    "| PRKAR1B       | cAMP-dependent protein kinase type I-beta *regulatory* subunit                 | Regulatory Subunit |\n",
    "| PRKAR2B       | cAMP-dependent protein kinase type II-beta *regulatory* subunit                | Regulatory Subunit |\n",
    "| RYR2          | *Ryanodine Receptor* 2 (24.3 kDa!)                                             | Substrate of PKA   |\n",
    "\n",
    "=> Exclude structures, that correspond to/comprise the regulatory subunit of PKA or represent complexes with protein substrates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce864a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of of gene names, that will be kept\n",
    "genenames_to_keep = ['PKA', 'PKACA', 'NaN', 'PKIA', 'PRKACN1', 'PKIA PRKACN1']\n",
    "\n",
    "# Assign all other gene names to the genenames_to_drop list\n",
    "# (we are not interested in the latter, as they resemble the regulatory subunit or protein substrates) \n",
    "genenames_to_drop = [i for i in genenames if i not in genenames_to_keep]\n",
    "\n",
    "print('Gene Names we are not interested in, as they resemble the regulatory subunit or substrate proteins of PKA =')\n",
    "print(genenames_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the gene name 'PKA' is to generic, we check which structures are annotated with the latter \n",
    "pdb_info_df_PKAgenename = pdb_info_df[pdb_info_df['Gene_Name'] == 'PKA'] \n",
    "# Show all rows for these PDB entries\n",
    "pdb_info_df[pdb_info_df['PDB_ID'].isin(pdb_info_df_PKAgenename['PDB_ID'].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44850a47",
   "metadata": {},
   "source": [
    "These 3 PDB-IDs (3E8C, 3E8E, 3MVJ) are all of PKA-catalytic subunit alpha structures. For all, another row with gene_name = PKACA is present in the data frame, for which all other info is the same. Therefore, we can simply kick out the entries with gene_Name = PKA, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pdb_info_df))\n",
    "\n",
    "# Keep only rows, for which the Gene_Name column does not equal to the generic 'PKA' annotation\n",
    "pdb_info_df = pdb_info_df[pdb_info_df['Gene_Name'] != 'PKA']\n",
    "pdb_info_df = pdb_info_df.reset_index(drop=True)\n",
    "\n",
    "print(len(pdb_info_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6ae1e",
   "metadata": {},
   "source": [
    "## Exclude structures, that are/comprise the regulatory subunit or are substrates of PKA based on gene names\n",
    "Has to be performed on the PDB IDs, as there might be multiple rows per ID and they might differ on the gene name, e.g. if a peptidic inhibitor is bound. Hence, first extract the PDB IDs to be excluded based on gene name, then exclude all entries with the given PDB IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with a boolean values, that indicates whether the Gene_Name is in the genenames_to_drop list or not \n",
    "pdb_info_df['Gene_Name_Bool'] = pdb_info_df['Gene_Name'].isin(genenames_to_drop)\n",
    "\n",
    "# Query the data frame for rows where the Gene_Name_Bool column is True\n",
    "# and write the corresponding PDB-IDs to the pdb_ids_to_drop list\n",
    "pdb_ids_to_drop = sorted(list(set(pdb_info_df.query(\"Gene_Name_Bool == True\")['PDB_ID'])))\n",
    "print(f'{len(pdb_ids_to_drop)} PDB-IDs are not considered as a consequence of the gene name filtering = \\n {pdb_ids_to_drop} \\n')\n",
    "\n",
    "# Drop the helper column Gene_Name_Bool from the data frame again\n",
    "pdb_info_df.drop('Gene_Name_Bool', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772e8bd",
   "metadata": {},
   "source": [
    "**Manual Cross-Check via PDB GUI**\n",
    "\n",
    "PDB-ID | Reason for exclusion | \n",
    "| --- | --- |\n",
    "2QCS | contains a regulatory subunit | \n",
    "3FHI | contains a regulatory subunit | \n",
    "3IDB | contains a regulatory subunit | \n",
    "3IDC | contains a regulatory subunit | \n",
    "3O7L | in complex with phospholamban-derived peptide | \n",
    "3PVB | contains a regulatory subunit | \n",
    "4DIN | contains a regulatory subunit | \n",
    "4WBB | contains a regulatory subunit | \n",
    "4X6R | contains a regulatory subunit | \n",
    "5JR7 | contains a regulatory subunit | \n",
    "6MM6 | in complex with RyR2 | \n",
    "6MM7 | in complex with RyR2 | \n",
    "6MM8 | in complex with RyR2 | \n",
    "6NO7 | contains a regulatory subunit | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now finally filter the pdb_info_df to all rows, for which the PDB-ID *is not* in the pdb_ids_to_drop list \n",
    "pdb_info_df = pdb_info_df[~pdb_info_df['PDB_ID'].isin(pdb_ids_to_drop)]\n",
    "pdb_info_df = pdb_info_df.reset_index(drop=True)\n",
    "\n",
    "# Cross-Check, which gene names are included after the gene name filtering\n",
    "print(f'Gene Names after Filtering = \\n {list(pdb_info_df.Gene_Name.unique())} \\n ')\n",
    "\n",
    "print(f'Length of the data frame after Gene Name filtering = {len(pdb_info_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = pdb_info_df[pdb_info_df['Gene_Name'] == 'NaN'] \n",
    "# row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454d48b",
   "metadata": {},
   "source": [
    "## Structures that also comprise another polymer entity (besides PKA)\n",
    "Add a column named 'Multiple_Polymer_Entities' to the pdb_info_df, <br> whose value is 'True' when a peptidic inhibitor is present, else 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50129d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new column 'Peptidic_Inhibitor' and set it to False by default\n",
    "pdb_info_df['Peptidic_Inhibitor'] = False\n",
    "\n",
    "# Iterate over pdb_info_df, that was grouped by PDB-ID and\n",
    "# Update 'Peptidic_Inhibitor' to True if a PDB IDs that occur multiple times, i.e. if another polymer entity than PKACA is present. \n",
    "for pdb_id, group in pdb_info_df.groupby('PDB_ID'):\n",
    "    if len(group) > 1:\n",
    "        pdb_info_df.loc[pdb_info_df['PDB_ID'] == pdb_id, 'Peptidic_Inhibitor'] = True\n",
    "\n",
    "# Write data frame to file\n",
    "pdb_info_df.to_html(os.path.join(tmp_folder, f'PDB_PKA_pdb_info_after_genename_filtering.html'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af366ca",
   "metadata": {},
   "source": [
    "## Extract only the rows for the PKACA entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1674281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame, that only contains the rows for the PKA Protein and not for the other polymer entities\n",
    "pka_df = pdb_info_df[pdb_info_df['EC_Number'] == '2.7.11.11'] \n",
    "pka_df = pka_df.reset_index(drop=True)\n",
    "\n",
    "print(len(pka_df))\n",
    "pka_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc54dd",
   "metadata": {},
   "source": [
    "### Cross-Check: Does any PDB ID still occur multiple times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a new DataFrame that only includes the rows where the PDB_ID column has a duplicate value \n",
    "# without altering the original pka_df DataFrame.\n",
    "pka_df[pka_df['PDB_ID'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532592b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data frame to file\n",
    "pka_df.to_excel(os.path.join(tmp_folder, f'PDB_PKA_after_genename_filtering.xlsx'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ef1ee",
   "metadata": {},
   "source": [
    "### PDB-IDs (after gene name filtering, only ligand-bound structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16210b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the PDB-IDs (after gene name filtering and exclusion of structure not comprising any non-polymer entity) \n",
    "# to a list named pdb_ids_pka\n",
    "pdb_ids_pka = list(pka_df.PDB_ID.unique())\n",
    "print(f'{len(pdb_ids_pka)} PDB-IDs after Gene Name and filtering exclusion of structure not comprising any non-polymer entity = \\n {pdb_ids_pka} \\n ')\n",
    "\n",
    "# Save the pdb_ids_pka list to file\n",
    "with open(os.path.join(tmp_folder, f'PDB_PKA_pdb_ids_after_genename_filtering.txt'), 'w') as f:\n",
    "    f.writelines(\"%s\\n\" % pdb_id for pdb_id in pdb_ids_pka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pka_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a1475",
   "metadata": {},
   "source": [
    "# Retrieve the Ligands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a838f",
   "metadata": {},
   "source": [
    "Now, get the ligand information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da55302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pdb_ligand_info(pdb_id):\n",
    "    \"\"\"\n",
    "    Fetch non-polymer data from rcsb.org.\n",
    "    \n",
    "    Parameters: \n",
    "        pdb_id (str): The PDB ID of the structure to be queried.\n",
    "\n",
    "    Returns: \n",
    "        info (dict):  A dictionary containing information about the PDB structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the GraphQL query, requesting various information about the ligand(s), as a multiline string.\n",
    "    query = (\n",
    "       \"\"\"{ entry(entry_id: \"%s\") {     \n",
    "    rcsb_id\n",
    "    nonpolymer_entities {\n",
    "      rcsb_nonpolymer_entity_container_identifiers {nonpolymer_comp_id rcsb_id}\n",
    "      nonpolymer_comp {\n",
    "        chem_comp {formula formula_weight id name}\n",
    "        rcsb_chem_comp_descriptor { SMILES SMILES_stereo }\n",
    "        rcsb_chem_comp_related { resource_accession_code resource_name } }\n",
    "      rcsb_nonpolymer_entity_annotation {type}\n",
    "      rcsb_nonpolymer_entity_container_identifiers {entity_id rcsb_id}\n",
    "      nonpolymer_entity_instances {\n",
    "        rcsb_nonpolymer_entity_instance_container_identifiers {asym_id auth_asym_id auth_seq_id entity_id} \n",
    "        rcsb_nonpolymer_instance_validation_score {\n",
    "                    RSCC\n",
    "                    RSR\n",
    "                    alt_id\n",
    "                    completeness\n",
    "                    intermolecular_clashes\n",
    "                    is_best_instance\n",
    "                    mogul_angle_outliers\n",
    "                    mogul_angles_RMSZ\n",
    "                    mogul_bond_outliers\n",
    "                    mogul_bonds_RMSZ\n",
    "                    ranking_model_fit\n",
    "                    ranking_model_geometry\n",
    "                    score_model_fit\n",
    "                    score_model_geometry\n",
    "                    stereo_outliers\n",
    "                    average_occupancy\n",
    "                    type\n",
    "                    is_subject_of_investigation\n",
    "                    is_subject_of_investigation_provenance}\n",
    "                    } } } }    \n",
    "        \"\"\"\n",
    "        % pdb_id\n",
    "    )\n",
    "\n",
    "    # Create the query URL by appending the query to the RCSB PDB API base URL.\n",
    "    query_url = f\"https://data.rcsb.org/graphql?query={query}\"\n",
    "    \n",
    "    # Send the GET request to the RCSB PDB API and retrieve the response.\n",
    "    response = requests.get(query_url)\n",
    "    \n",
    "    # Raise an exception if the request failed.\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse the JSON response and store it in the 'info' variable.\n",
    "    info = response.json()\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the PDB IDs in the 'pdb_ids_pka' list \n",
    "# Call the 'retrieve_pdb_ligand_info' function to get further info on each structure \n",
    "# Exploiting the tqdm module to display a progress bar\n",
    "lig_infos = [retrieve_pdb_ligand_info(pdb_id_pka) for pdb_id_pka in tqdm(pdb_ids_pka, desc=\"Retrieve PDB Ligand Infos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a69fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info_to_file(filename = os.path.join(tmp_folder, f\"PDB_API_LigandMetadata.txt\"), \n",
    "                  retrieve_info = retrieve_pdb_ligand_info, \n",
    "                  pdb_ids = pdb_ids_pka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c9c77",
   "metadata": {},
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b13ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lig_info_df(pdb_ids, lig_infos):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a data frame containing ligand information from the information retrieved via PDB API.  \n",
    "\n",
    "    Parameters:\n",
    "        pdb_ids   (List[str]):  List of PDB IDs.\n",
    "        lig_infos (List[dict]): List of dictionaries containing the ligand information.\n",
    "\n",
    "    Returns:\n",
    "        lig_info_df (pd.DataFrame): Dataframe containing PDB information on the ligands.\n",
    "\n",
    "    \"\"\"\n",
    "    info = []\n",
    "    for i, lig_info in enumerate(lig_infos):  \n",
    "                        \n",
    "        info.append({\n",
    "            'PDB_ID':          extract_from_dict(lig_info, \"rcsb_id\")[0],\n",
    "            'Lig_ID':          extract_from_dict(lig_info, \"id\"),\n",
    "            'Lig_Name':        extract_from_dict(lig_info, \"name\"),\n",
    "            'Lig_Formula':     extract_from_dict(lig_info, \"formula\"),\n",
    "            'Lig_SMILES':      extract_from_dict(lig_info, \"SMILES_stereo\"), # isomeric SMILES!\n",
    "            'Lig_MW':          extract_from_dict(lig_info, \"formula_weight\"),\n",
    "            'Lig_Database_ID': extract_from_dict(lig_info, \"rcsb_chem_comp_related\"),\n",
    "            'Lig_Entity_ID':   extract_from_dict(lig_info, \"rcsb_nonpolymer_entity_container_identifiers\"),\n",
    "            'Lig_Quality':     extract_from_dict(lig_info, \"nonpolymer_entity_instances\")\n",
    "        })\n",
    "        \n",
    "    lig_info_df = pd.DataFrame(info)\n",
    "\n",
    "    return lig_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the create_lig_info_df function to create a pandas.data frame containing the ligand information\n",
    "lig_info_df = create_lig_info_df(pdb_ids_pka, lig_infos)\n",
    "print(len(lig_info_df))\n",
    "lig_info_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff138251",
   "metadata": {},
   "source": [
    "## Post-processing of the lig_info_df data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Entity ID for the ligand \n",
    "lig_info_df['Lig_Entity_ID'] = extract_values_from_dict_in_column(lig_info_df, 'Lig_Entity_ID', 'rcsb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns that require explosion, i.e. transformation of the column lists into individual rows\n",
    "explosion_list = ['Lig_ID', 'Lig_Name', 'Lig_Formula', 'Lig_SMILES', 'Lig_MW' , 'Lig_Database_ID', 'Lig_Entity_ID', 'Lig_Quality'] \n",
    "\n",
    "# Explode the data frame based on the explosion_list, i.e. transform each element of the column lists into a row\n",
    "# Thereby, ignore the index and create a new one.\n",
    "lig_info_df = lig_info_df.explode(explosion_list, ignore_index=True)\n",
    "lig_info_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf096fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a list of cross-references to other databases\n",
    "# by string joining the resource_name and resource_accession_code together, using the : separator \n",
    "lig_info_df['Lig_Database_ID'] = lig_info_df['Lig_Database_ID'].apply(\n",
    "    lambda x: [f\"{d['resource_name']}: {d['resource_accession_code']}\" for d in x] if x is not None else [])\n",
    "\n",
    "# Now we are able to extract the IDs from other databases, such as ChEMBL or drugbank, etc. to individual columns\n",
    "def extract_database_ids(df, database_name):\n",
    "    # Create a new column in the dataframe with the extracted database IDs\n",
    "    df[f'Lig_{database_name}_ID'] = df['Lig_Database_ID'].apply(\n",
    "        lambda x: \n",
    "            str(sorted(list(set([\n",
    "                # Split each item in the list x by ': ' and take the second part \n",
    "                item.split(': ')[1] for item in x \n",
    "                # Filter items that start with the database name (case-insensitive)\n",
    "                if item.lower().startswith(database_name.lower() + ':') \n",
    "            ]))))\n",
    "            # If x is not None and contains at least one item that starts with the database name \n",
    "            if x and any(item.lower().startswith(database_name.lower() + ':') for item in x) \n",
    "            # If x is None or empty, return a list containing 'NaN'\n",
    "            else 'NaN')\n",
    "    return df\n",
    "\n",
    "# Loop through each database and extract the IDs\n",
    "for database in ['ChEMBL', 'DrugBank']:\n",
    "    lig_info_df = extract_database_ids(lig_info_df, database)\n",
    "\n",
    "# Drop the helper column\n",
    "lig_info_df = lig_info_df.drop(columns=['Lig_Database_ID'])\n",
    "\n",
    "lig_info_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from the strings in the Lig_Formula column\n",
    "lig_info_df['Lig_Formula'] = lig_info_df['Lig_Formula'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lig_info_df[lig_info_df['PDB_ID'] == '5N3A'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig_info_df.to_csv(os.path.join(tmp_folder, f'PDB_API_LigandMetadata.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f414e",
   "metadata": {},
   "source": [
    "## Group by \"Ligands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the 'Lig_ID' column\n",
    "# Then apply the list function to the 'PDB_ID' and the Lig_Entity_ID columns\n",
    "# Reset the index to get 'PDB_ID' as a column\n",
    "ligands_df = lig_info_df.groupby('Lig_ID').agg({\n",
    "    'PDB_ID':        list,\n",
    "    'Lig_Entity_ID': list\n",
    "}).reset_index()\n",
    "\n",
    "print(f'{len(ligands_df)} different ligands have been identified initially based on Ligand ID.') \n",
    "\n",
    "# Merge the grouped dataframe again with the other columns of the original dataframe\n",
    "ligands_df = ligands_df.merge(lig_info_df.copy().drop(['PDB_ID','Lig_Entity_ID', 'Lig_Quality'], axis=1).drop_duplicates(), \n",
    "                              on='Lig_ID', how='left')\n",
    "\n",
    "# Sort the 'ligands_df' data frame in ascending order based on 'Lig_MW' and reset the index\n",
    "ligands_df = ligands_df.sort_values(by=['Lig_MW'], ascending=True)\n",
    "ligands_df = ligands_df.reset_index(drop=True) \n",
    "\n",
    "# Record the length of the list in the PDB_ID column in the newly created 'Occurence' column\n",
    "# i.e. how often was a particular ligand co-crystallized?\n",
    "ligands_df['Occurence'] = ligands_df['PDB_ID'].apply(len)\n",
    "\n",
    "# Add molecule column \n",
    "PandasTools.AddMoleculeColumnToFrame(ligands_df, smilesCol=\"Lig_SMILES\", molCol='Lig_Structure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.RenderImagesInAllDataFrames(True) # to overcome display error\n",
    "ligands_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bfd52",
   "metadata": {},
   "source": [
    "## Uncharge/Neutralize the Molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/chembl/ChEMBL_Structure_Pipeline/blob/master/chembl_structure_pipeline/standardizer.py#L291\n",
    "\n",
    "def uncharge_mol(m):\n",
    "\n",
    "    \"\"\"\n",
    "    Uncharge molecules by adding and/or removing hydrogens.\n",
    "        - For zwitter ions, hydrogens are moved to eliminate charges where possible.\n",
    "        - By default, in cases where there is a positive charge that is not neutralizable, \n",
    "          an attempt is made to also preserve the corresponding negative charge.\n",
    "        - When the force option is set, all neutralizable sites are uncharged, \n",
    "          also when non-neutralizable positive charges are present and the resulting overall charge is therefore not null.\n",
    "    \"\"\"\n",
    "    # Uncharger is an RDKit algorithm that can neutralize charges in a molecule. \n",
    "    uncharger = rdMolStandardize.Uncharger(canonicalOrder=True) #force=False\n",
    "\n",
    "    # Apply the uncharger algorithm\n",
    "    res = uncharger.uncharge(m)\n",
    "\n",
    "    # Update the property cache of the resulting molecule to ensure accurate property calculations\n",
    "    res.UpdatePropertyCache(strict=False)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb24f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the uncharge function\n",
    "ligands_df['Lig_Structure_Uncharged'] = ligands_df['Lig_Structure'].apply(uncharge_mol)\n",
    "# Convert the uncharged molecule to SMILES \n",
    "ligands_df['Lig_SMILES_Uncharged'] = ligands_df['Lig_Structure_Uncharged'].apply(Chem.MolToSmiles)\n",
    "ligands_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data frame to a html file\n",
    "ligands_df.to_html(os.path.join(tmp_folder, f\"PDB_PKA_Ligands.html\"), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91bd16",
   "metadata": {},
   "source": [
    "## Create a subdata frame with molecules, that are \"no proper ligands\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75e259",
   "metadata": {},
   "source": [
    "Exclude Molecules contained in the USAN Councilâ€™s list of pharmacological salts (Bento et al. 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the salts file into a pandas data frame and subsequently extract the SMILES strings into a list\n",
    "salts_url = \"https://raw.githubusercontent.com/chembl/ChEMBL_Structure_Pipeline/master/chembl_structure_pipeline/data/salts.smi\"\n",
    "salts_df = pd.read_csv(salts_url, sep=\"\\t\", names=[\"Name\",\"SMILES\"], header=None)\n",
    "\n",
    "# Fix erroneous SMILES! \n",
    "salts_df.loc[salts_df['Name'] == 'Orotic acid', 'SMILES'] = 'C1=C(NC(=O)NC1=O)C(=O)O'\n",
    "salts_df.loc[salts_df['Name'] == 'Nitrate', 'SMILES']     = '[N+](=O)([O-])[O-] '\n",
    "\n",
    "PandasTools.AddMoleculeColumnToFrame(salts_df, smilesCol=\"SMILES\", molCol='Structure')\n",
    "\n",
    "# Uncharge the salts (the same way as done before for the PDB extracted ligands)\n",
    "salts_df['Structure_Uncharged'] = salts_df['Structure'].apply(uncharge_mol)\n",
    "salts_df['SMILES_Uncharged'] = salts_df['Structure_Uncharged'].apply(Chem.MolToSmiles)\n",
    "\n",
    "# Compute molecular weight of salts and sort the dataframe by the latter in ascending order\n",
    "salts_df['MW'] = salts_df['Structure'].apply(Descriptors.ExactMolWt)\n",
    "salts_df = salts_df.sort_values(by=['MW'], ascending=True, na_position='first', ignore_index=True)\n",
    "\n",
    "# Manually remove 'benzoate' from the salts_df, as it was found to be a ligand for PKA \n",
    "# (PDB-ID: 6SNN, published in https://doi.org/10.1002/anie.202011295 (Oebbeke et al 2020), hinge binder!)\n",
    "salts_df = salts_df[salts_df.Name != 'Benzoate']\n",
    "\n",
    "# Save to files \n",
    "salts_df.to_html(os.path.join(tmp_folder,f\"Salts.html\"), header=True)\n",
    "salts_df.to_html(os.path.join(tmp_folder, f\"Salts.csv\"), header=True)\n",
    "print(len(salts_df))\n",
    "\n",
    "# Save SMILES to list\n",
    "salts_smiles_uncharged = salts_df[\"SMILES_Uncharged\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe ligands_kicked_df for the molecules, that are \"no proper ligands\"\n",
    "# based on the  USAN Councilâ€™s list of pharmacological salts and manually identified molecules (grouped by molecule type)\n",
    "\n",
    "ligands_kicked_df = ligands_df[\n",
    "    # All molecules that posses only a one or two letter molecular formula = ions \n",
    "    (ligands_df['Lig_Formula'].map(len) < 3) |\n",
    "    # Pharmacological salts as defined by the USAN Council\n",
    "    (ligands_df.Lig_SMILES_Uncharged.isin(salts_smiles_uncharged)) |\n",
    "    # Molecules, identified by manual inspection\n",
    "    (ligands_df.Lig_ID.isin([\n",
    "        # glycols / diols\n",
    "        'EDO', 'BUD', 'PEG', 'MRD', 'MPD', '1PE', 'PGE', 'PG4', 'PG5', 'PG6',\n",
    "        # sugars\n",
    "        'RIP',   \n",
    "        # fatty acids, lipids, lipid-analogs and detergents                                  \n",
    "        'MYR', 'MG8', 'ZEU',\n",
    "        # solvents and biochemical buffers \n",
    "        'MOH', 'DMS', 'MES',\n",
    "        # amino acids \n",
    "        'GLY', 'SER', 'PRO', 'THR', 'DAR',\n",
    "        ])) \n",
    "    ]\n",
    "\n",
    "# Sort the ligands_kicked_df by molecular weight and drop duplicates                                       \n",
    "ligands_kicked_df = ligands_kicked_df.sort_values(by=['Lig_MW'], ascending=True, na_position='first', ignore_index=True)\n",
    "ligands_kicked_df = ligands_kicked_df.drop_duplicates(subset=['Lig_ID'], keep='first')\n",
    "\n",
    "print(f\"{len(ligands_kicked_df)} Molecules will be disregarded, as they are 'no proper ligands'.\")\n",
    "\n",
    "ligands_kicked_df.to_html(os.path.join(tmp_folder, f'PDB_PKA_Ligands_Kicked.html'), header=True)\n",
    "\n",
    "ligands_kicked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589f1e8",
   "metadata": {},
   "source": [
    "## Filter the list of ligands, based on the ligand defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the  ligands_df based on the ligands_kicked_df\n",
    "# i.e. keep only the ligands that are not contained in the ligands_kicked_df\n",
    "ligands_df_filtered = ligands_df[~ligands_df['Lig_ID'].isin(ligands_kicked_df['Lig_ID'])]\n",
    "ligands_df_filtered = ligands_df_filtered.drop_duplicates(subset=['Lig_ID'], keep='first') \n",
    "ligands_df_filtered = ligands_df_filtered.reset_index(drop=True)\n",
    "\n",
    "print(f'After filtering, {len(ligands_df_filtered)} different ligands have been identified.')\n",
    "\n",
    "ligands_df_filtered.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files \n",
    "ligands_df_filtered.to_html(os.path.join(filepath, f'PDB_PKA_Ligands_Filtered.html'), header=True, index=False) # FINAL FILE\n",
    "ligands_df_filtered.copy().drop(['Lig_Structure', 'Lig_Structure_Uncharged'], axis=1).to_excel(os.path.join(filepath, f'PDB_PKA_Ligands_Filtered.xlsx'), header=True, index=False) # FINAL FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ff47a",
   "metadata": {},
   "source": [
    "### Cross-Checking: Check for structures, comprising more than one ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6672570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'PDB_ID' column to create a separate row for each PDB ID\n",
    "# (before the data was structured per ligand)\n",
    "ligands_df_filtered_exploded = ligands_df_filtered.explode(column=['PDB_ID', 'Lig_Entity_ID'])\n",
    "\n",
    "# Sort the dataframe by the 'PDB_ID' column and reset the index\n",
    "ligands_df_filtered_exploded = ligands_df_filtered_exploded.sort_values(by='PDB_ID').reset_index(drop=True)\n",
    "\n",
    "# Count the occurrence of each PDB_ID in the data frame\n",
    "pdb_id_counts = ligands_df_filtered_exploded['PDB_ID'].value_counts()\n",
    "\n",
    "# Get the PDB_IDs that have a count greater than 1, i.e. for which multiple 'ligands' are present\n",
    "multilig_pdb_ids = pdb_id_counts[pdb_id_counts > 1].index\n",
    "\n",
    "if len(multilig_pdb_ids) == 0: \n",
    "    print('After filtering, no PDB structure comprises more than 1 ligand.')\n",
    "    multilig_df = None\n",
    "    \n",
    "else: \n",
    "    print('Please note that some structures comprise multiple ligands matching our ligand definition. \\n',\n",
    "          'You may wish to revise the \"ligand definition\" employed, before you continue.')\n",
    "    # Select rows in the data frame where the PDB_ID is not unique, i.e. for which multiple 'ligands' are present\n",
    "    multilig_df = ligands_df_filtered_exploded[ligands_df_filtered_exploded['PDB_ID'].isin(multilig_pdb_ids)]\n",
    "    \n",
    "multilig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15345c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the the PDB IDs in the dataframe to a list\n",
    "pdb_ids_pka_ligands = ligands_df_filtered_exploded['PDB_ID'].to_list()\n",
    "\n",
    "# Write to file\n",
    "with open(os.path.join(tmp_folder, f'PDB_PKA_pdb_ids_after_genename_and_ligand_filtering.txt'), 'w') as f: \n",
    "    f.writelines(\"%s\\n\" % pdb_id for pdb_id in pdb_ids_pka_ligands)\n",
    "\n",
    "print(f\"The following PDB-IDs are not considered, as they do not comprise a ligand matching our definition:\")\n",
    "print(sorted(list(set(pdb_ids_pka).difference(set(pdb_ids_pka_ligands)))))\n",
    "print(f\"({(len(pdb_ids_pka) - len(pdb_ids_pka_ligands))} PDB structures)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924707f",
   "metadata": {},
   "source": [
    "**Manual cross-checking (via PDB website) validates this decision.**\n",
    "| PDB-ID  | \"No proper Ligand(s)\"                                                                | Comment |\n",
    "|---------|--------------------------------------------------------------------------------------|---------|\n",
    "| 1CMK    | iodide ion [IOD] and myristic   acid [MYR] = is myristoylated                        |\n",
    "| 4AE6    | acetate ion [ACT]                                                                    |\n",
    "| 4DFZ    | myristic acid [MYR] = is   myristoylated                                             |\n",
    "| 4DG2    | myristic acid [MYR] = is   myristoylated                                             |\n",
    "| 4NTS    | myristic acid  [MYR]                                                                 |\n",
    "| 5IZF    | sulfate ion [SO4]                                                                    |\n",
    "| 5M0U    | 4R)-2-methylpentane-2,4-diol [MRD] and methanol [MOH]                                |\n",
    "| 5N3M   | dimethylsulfoxide [DMS],   (4S)-2-methylpentane-2,4-diol [MPD] and D-Arginine [DAR]  | Dissertation C. Siefker 2018 |\n",
    "| 5NTJ  | beta-D-ribopyranose [RIP] and   (4S)-2-Methyl-2,4-pentanediol [MPD]                  | [MÃ¼ller et al 2019](https://doi.org/10.1021/acsomega.8b02364) |\n",
    "| 5OUA  | beta-D-ribopyranose [RIP]                                                            | [MÃ¼ller et al 2019](https://doi.org/10.1021/acsomega.8b02364) |\n",
    "| 5OUS  | (4S)-2-methylpentane-2,4-diol [MPD]                                                  | [MÃ¼ller et al 2019](https://doi.org/10.1021/acsomega.8b02364) |\n",
    "| 6ERV  | (4S)-2-methylpentane-2,4-diol [MPD]                                                  | [MÃ¼ller et al 2019](https://doi.org/10.1021/acsomega.8b02364) |\n",
    "\n",
    "Is D-Arginine in 5N3M a \"proper ligand\"? <br> \n",
    "According to the primary literature, the D-Arg binds in the same position as the Arg19 of a PKI, as the structure was obtained in a PKI-free form. Hence, we can ignore this structure safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pdb_ids_pka_ligands), 'PDB structures comprise a small molecule ligand matching our \"ligand definition\"')\n",
    "print(f'(out of {len(pdb_ids)}   PDB-Structures that were initially identified for our Protein of Interest): \\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending by Lig_ID and descending by Occurence and reset the index\n",
    "ligands_df_filtered_exploded = ligands_df_filtered_exploded.sort_values(by= ['Lig_ID', 'Occurence'], ascending=[True, False]).reset_index(drop=True)\n",
    "ligands_df_filtered_exploded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Lig IDs in the filtered data frame to a list\n",
    "lig_ids = ligands_df_filtered_exploded['Lig_ID'].to_list()\n",
    "# Sort them alphabetically\n",
    "lig_ids = sorted(lig_ids)\n",
    "print(*lig_ids)\n",
    "\n",
    "# Save list to file\n",
    "with open(os.path.join(filepath, f'PDB_PKA_ligand_ids.txt'), 'w') as f: # FINAL FILE\n",
    "    f.writelines(\"%s\\n\" % lig_id for lig_id in lig_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f157f",
   "metadata": {},
   "source": [
    "# Quality of Ligand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Ligand Quality Data, that was already contained in the lig_info_df to a new dataframe.\n",
    "\n",
    "# Data Structure of the Lig_Quality column before: \n",
    "    # List, with one entry for each different type of ligand. \n",
    "    # Each entry possesses two dictionaries, rcsb_nonpolymer_entity_instance_container_identifiers & rcsb_nonpolymer_instance_validation_score\n",
    "    # The latter stores the actual ligand quality measures, again structures as a list of dictionaries, per ligand copy.\n",
    "\n",
    "# Explosion to create an individual row per different non-polymer instance  \n",
    "# Data Structure after: one row/dictionary for each different type of ligand\n",
    "lig_validation_df = lig_info_df.explode(['Lig_Quality'], ignore_index=True)\n",
    "\n",
    "# Extract the actual ligand quality measures data\n",
    "# Data structure after: list of dictionaries, with a dictionary per ligand copy\n",
    "lig_validation_df['Lig_Quality'] = extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'rcsb_nonpolymer_instance_validation_score')\n",
    "\n",
    "# Explode, to create an individual row per ligand copy\n",
    "# Data structure after: Simple dictionary encoding the ligand quality measures\n",
    "lig_validation_df = lig_validation_df.explode(['Lig_Quality'], ignore_index=True)\n",
    "\n",
    "# Extract the measures to individual columns, using the extract_values_from_dict_in_column function and the respective keyword\n",
    "# Data structure after: simple numeric values, one row per copy of a ligand present in the PDB structure\n",
    "lig_validation_df['Lig_Conformer'] =      extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'alt_id')\n",
    "lig_validation_df['Lig_Of_Interest'] =    extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'is_subject_of_investigation')\n",
    "lig_validation_df['Lig_Completeness'] =   extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'completeness')\n",
    "lig_validation_df['Lig_Occupancy'] =      extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'average_occupancy')\n",
    "lig_validation_df['Lig_RSCC'] =           extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'RSCC')\n",
    "lig_validation_df['Lig_RSR'] =            extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'RSR')\n",
    "lig_validation_df['Lig_RMSZ_Bonds'] =     extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'mogul_bonds_RMSZ')\n",
    "lig_validation_df['Lig_RMSZ_Angles'] =    extract_values_from_dict_in_column(lig_validation_df, 'Lig_Quality', 'mogul_angles_RMSZ')\n",
    "\n",
    "# Drop the original column after we extracted all the measures to individual columns\n",
    "lig_validation_df.drop('Lig_Quality', axis=1, inplace=True)\n",
    "\n",
    "# Regroup the DataFrame again by the 'Lig_Entity_ID' column,   \n",
    "# Thereby apply the list function to all the ligand quality related columns\n",
    "# Data structure after: List of numeric values, for each ligand (list length = 1 if only one copy of that ligand bound)\n",
    "lig_validation_df = lig_validation_df.groupby('Lig_Entity_ID').agg({\n",
    "    'Lig_Entity_ID' : 'first', \n",
    "    **{col: list for col in lig_validation_df.columns.to_list() if col in \n",
    "    ['Lig_Conformer', 'Lig_Of_Interest', 'Lig_Completeness', 'Lig_Occupancy', \n",
    "     'Lig_RSCC', 'Lig_RSR', 'Lig_RMSZ_Bonds', 'Lig_RMSZ_Angles']}, \n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "# Add a PDB_ID column to the dataframe\n",
    "lig_validation_df['PDB_ID'] = lig_validation_df['Lig_Entity_ID'].str[:4]\n",
    "\n",
    "# Recombine the ligand validation data with the filtered ligand dataframe\n",
    "# Thereby return only the rows, that have matching values in both dataframes (\"proper\" ligands only)\n",
    "ligands_df_filtered_exploded = ligands_df_filtered_exploded.merge(lig_validation_df, on=['Lig_Entity_ID', 'PDB_ID'], how='inner')\n",
    "\n",
    "# Store the number of alternative conformations (indicated by non-'NaN' values) in the Lig_Conformer column in new column\n",
    "ligands_df_filtered_exploded['Lig_Alt_Conf'] = ligands_df_filtered_exploded['Lig_Conformer'].apply(lambda x: len([i for i in x if i != 'NaN']))\n",
    "\n",
    "ligands_df_filtered_exploded.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9193619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store the number of different binding sites in a new column\n",
    "# # Thereby assume, that only one ligand copy can be bound per binding site and \n",
    "# # that the occupancy values of alternate conformations, binding at the site, should sum up to 1 (or less, if not fully occupied)\n",
    "# # Note, that this is just a rough estimate, that should be confirmed by inspecting the structure!\n",
    "# # Sum up the occupancy values for each different ligand type, and round up to the next integer value\n",
    "# # If x is not a list of numeric values, return 'NaN'\n",
    "\n",
    "# ligands_df_filtered_exploded['Lig_Sites'] = ligands_df_filtered_exploded['Lig_Occupancy'].apply(\n",
    "#     lambda x: math.ceil(sum(x)) if all(isinstance(i, (int, float)) for i in x) else 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files \n",
    "ligands_df_filtered_exploded.to_html( os.path.join(filepath, f'PDB_PKA_Ligands_Filtered_Exploded.html'), header=True) # FINAL FILE\n",
    "ligands_df_filtered_exploded.to_excel(os.path.join(filepath, f'PDB_PKA_Ligands_Filtered_Exploded.xlsx'), header=True) # FINAL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ccc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ligands_df_filtered_exploded[ligands_df_filtered_exploded['PDB_ID'] == '3VQH'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb97140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lig_validation_df[lig_validation_df['PDB_ID'] == '3VQH'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384f0c6",
   "metadata": {},
   "source": [
    "## Cross Check: Compare our Ligand Definition with the PDB's Ligand-Of-Interest (LOI) annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{lig_validation_df['Lig_Of_Interest'].apply(lambda x: 'Y' in x).sum()} 'Ligands Of Interest' Annotations are given in the PDB\") \n",
    "print(f\"(for {lig_validation_df['PDB_ID'].nunique()} ligand-bound PDB structures).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9447bf1",
   "metadata": {},
   "source": [
    "Investigate, if we excluded any ligand with LOI annotation, based on our ligand definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe, by merging the specified columns of the lig_info_df and lig_validation_df  \n",
    "loi_excluded = pd.merge(lig_info_df, \n",
    "                        lig_validation_df[['Lig_Entity_ID', 'Lig_Of_Interest']], \n",
    "                        on=['Lig_Entity_ID'], how='outer') \n",
    "\n",
    "# Include only the rows/ligands, that were kicked, as they do not match our ligand defintion\n",
    "loi_excluded = loi_excluded[loi_excluded['Lig_ID'].isin(ligands_kicked_df['Lig_ID'])]\n",
    "\n",
    "# Keep only rows, for which the LOI annotation equals to YES\n",
    "loi_excluded = loi_excluded[loi_excluded['Lig_Of_Interest'] == 'Y']\n",
    "\n",
    "print(\"\\n The ligands with 'Ligand Of Interest' annotation in the PDB, that we lost in the filtering steps are: \")\n",
    "print(f\"{loi_excluded['Lig_ID'].unique()} (from {loi_excluded['PDB_ID'].nunique()} different PDB structures)\")\n",
    "\n",
    "# loi_excluded = loi_excluded.groupby(['Lig_ID', 'Lig_Name', 'Lig_Of_Interest', 'Lig_SMILES', 'Lig_CHEMBL_ID']).agg({'PDB_ID': list}).reset_index()\n",
    "# loi_excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544cee4",
   "metadata": {},
   "source": [
    "Check, if we include any ligands in our analysis, for which no LOI annotation is present in the PDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the ligands_df_filtered_exploded dataframe to keep only the rows where 'Lig_Of_Interest' column value contains 'N'\n",
    "ligands_without_annotation = ligands_df_filtered_exploded[ligands_df_filtered_exploded['Lig_Of_Interest'].apply(lambda x: 'N' in x)]\n",
    "\n",
    "print(f\"{len(ligands_without_annotation)} ligands are included in my definition, that do not possess the\") \n",
    "print(f\"PDB 'Ligand Of Interest' Annotation, namely {list(ligands_without_annotation['Lig_ID'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9ca44",
   "metadata": {},
   "source": [
    "But is/are there any 'Ligand of Interest' Annotation(s) given in these PDB structures at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d92549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode all columns of the lig_validation_df, except the ID columns\n",
    "lig_validation_df_exploded = lig_validation_df.explode(list(set(lig_validation_df.columns) - set(['PDB_ID', 'Lig_Entity_ID'])), ignore_index=True)\n",
    "# Filter the ligands_df_filtered_exploded dataframe based on a list of PDB_IDs, whose associated structures fulfil the following 2 conditions\n",
    "# 1. The PDB_ID is present in ligands_without_annotation['PDB_ID'] dataframe, i.e. \n",
    "# 2. Any of the ligands has a PDB LOI annotation ('Y') \n",
    "ligands_df_filtered_exploded[ligands_df_filtered_exploded['PDB_ID'].isin(lig_validation_df_exploded['PDB_ID'][(lig_validation_df_exploded['PDB_ID'].isin(ligands_without_annotation['PDB_ID'])) & (lig_validation_df_exploded['Lig_Of_Interest'] == 'Y')].unique())] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec384cc8",
   "metadata": {},
   "source": [
    "# Combine structure and ligand information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"master data frame\" comprising all info, by merging the dataframes pka_df and ligands_df_filtered_exploded on 'PDB_ID'\n",
    "# Merging method \"Left\" considers only the PDB_IDs, present in the ligands_df_filtered_exploded dataframe\n",
    "# As the ligands_df_filtered_exploded is already limited to the ligands matching our ligand definition, so is the resulting df\n",
    "df = ligands_df_filtered_exploded.merge(pka_df, on='PDB_ID', how='left')\n",
    "\n",
    "# Convert values in Lig_ID column to strings\n",
    "df['Lig_ID'] = df['Lig_ID'].astype(str)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331903b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Lig_Missing_Atoms' to indicate if ligand atoms are missing\n",
    "# by checking if the Lig_Completeness value is < 1 \n",
    "df['Lig_Missing_Atoms'] = df['Lig_Completeness'].apply(lambda x: ['Yes' if i > 1 else 'No' for i in x if isinstance(i, (int, float))] if any(isinstance(i, (int, float)) for i in x) else ['NaN'])\n",
    "\n",
    "incomplete_ligs = [pdb_id for pdb_id, lig_missing_atoms in zip(df['PDB_ID'], df['Lig_Missing_Atoms']) if 'Yes' in lig_missing_atoms]\n",
    "# incomplete_ligs.sort()\n",
    "\n",
    "print(f\"According to the ligand quality measure 'Lig_Completeness', in {len(incomplete_ligs)} complex structures the ligand of interest is not fully resolved.\")\n",
    "print(*incomplete_ligs)\n",
    "\n",
    "print(f\"For comparison, the REMARK 610 entry reports missing atoms for {sum(row['Lig_ID'] in row['REMARK_610'] for index, row in df.iterrows())} of them.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430177c",
   "metadata": {},
   "source": [
    "# Add Binding Data from the PDBBind Database\n",
    "(as obtained from the 'PDBbind/PDBbind_V2020.ipynb' JupyterNotebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the beautified table containing the data from the PDBbind Database\n",
    "PDBbind_path = os.path.join(filepath, 'PDBbind')\n",
    "PDBbind_df = pd.read_csv(os.path.join(PDBbind_path, 'PDBbind_V2020.csv'), sep=\"|\")\n",
    "\n",
    "# Rename column names, except the PDB_ID column, by using upper case letters \n",
    "PDBbind_df.columns = [col if col == 'PDB_ID' else col.replace('_', '_').title() for col in PDBbind_df.columns]\n",
    "\n",
    "# Add the prefix 'PDBbind_' to all column names\n",
    "PDBbind_df.columns = ['PDBbind_' + col if col != 'PDB_ID' else col for col in PDBbind_df.columns]\n",
    "\n",
    "# Rename the 'PDBbind_Ligand_Name' column\n",
    "PDBbind_df = PDBbind_df.rename(columns={'PDBbind_Ligand_Name': 'Lig_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd262fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the info from the PDBbind_df to the \"master dataframe\" df\n",
    "# Merging method \"Left\" considers only the PDB_IDs, present in the dataframe df\n",
    "df = df.merge(PDBbind_df, on=['PDB_ID', 'Lig_ID'], how='left')\n",
    "\n",
    "binding_data_df = df[df['PDBbind_Standard_Value'].notna()]\n",
    "print(f'For {len(binding_data_df)} out of {len(df)} complex structures, a binding affinity was reported for in the PDBbind database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b219f",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_html(os.path.join(filepath, 'PDB_PKA_StructuralDetails_Ligands.html'), header=True, index=False) # FINAL FILE\n",
    "\n",
    "# Create a copy of the data frame, drop the columns ['Lig_Structure', 'Lig_Structure_Uncharged']from the copy and save to excel file\n",
    "df.copy().drop(['Lig_Structure', 'Lig_Structure_Uncharged'], axis=1).to_excel(os.path.join(filepath, 'PDB_PKA_StructuralDetails_Ligands.xlsx'), header=True, index=False) # FINAL FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c846e",
   "metadata": {},
   "source": [
    "# Fetch pdb structures / files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876f86d",
   "metadata": {},
   "source": [
    "the following code was adopted from https://www.macinchem.org/reviews/Data/PDB/downloadpdb.php and adapted for our purposes\n",
    "- Authored by Chris Swain\n",
    "- Copyright CC-BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pdb_ids_pka_ligands))\n",
    "print(*pdb_ids_pka_ligands)\n",
    "\n",
    "# Save alphabetically-sorted list to file\n",
    "with open(os.path.join(filepath, f'PDB_PKA_pdb_ids.txt'), 'w') as f: # FINAL FILE\n",
    "    f.writelines(\"%s\\n\" % pdb_id for pdb_id in sorted(pdb_ids_pka_ligands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to download files to\n",
    "download_folder = os.path.join(filepath, 'pdb_files')\n",
    "\n",
    "# Whether to download gzip compressed files (True) or unpacked\n",
    "compressed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure download folder exists\n",
    "try:\n",
    "    os.makedirs(download_folder)\n",
    "except OSError as e:\n",
    "    # Ignore OSError raised if it already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93961b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_download = [] #create empty list, where pdb-codes for which the download failed will be appended subsequently\n",
    "    \n",
    "for pdb_id in pdb_ids_pka_ligands:\n",
    "    # Add .pdb extension and remove ':1' suffix if entities\n",
    "    filename = '%s.pdb' % pdb_id[:4]\n",
    "    # Add .gz extension if compressed\n",
    "    if compressed:\n",
    "        filename = '%s.gz' % filename   \n",
    "    \n",
    "    destination_file = os.path.join(download_folder, filename)\n",
    "    \n",
    "    # Check if the file is already downloaded\n",
    "    if not os.path.isfile(destination_file):\n",
    "        try:\n",
    "            url = 'https://files.rcsb.org/download/%s' % filename\n",
    "            # Download the file\n",
    "            urlretrieve(url, destination_file)\n",
    "        except OSError:\n",
    "            failed_download.append(f'{pdb_id}') # e.g. because there is no corresponding file (.pdb or .pdb.gz) deposited in the PDB\n",
    "    \n",
    "print(f'Download of {failed_download} failed.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad62d6",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Exp_Method'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6048e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distribution(df, metric, unit):\n",
    "\n",
    "    # Save numerial, non-nan values to list and sort\n",
    "    values = sorted([i for i in df[metric].tolist() if isinstance(i, (float, int)) and not math.isnan(i)])\n",
    "\n",
    "    # Print Range\n",
    "    print(f'\\n{metric} {unit} ranges from {values[0]} to {values[-1]} ')\n",
    "\n",
    "    # Print Median\n",
    "    print(f'{metric} median {unit} = {np.median(values)}')\n",
    "\n",
    "    # Print PDB-ID(s) associated with lowest value\n",
    "    lowest = df.loc[df[metric] == values[-1], 'PDB_ID'].values\n",
    "    print(f'The lowest {metric} value is reported for PDB-ID(s): {lowest}')\n",
    "\n",
    "    # Print PDB-ID(s) associated with highest value\n",
    "    highest = df.loc[df[metric] == values[0], 'PDB_ID'].values\n",
    "    print(f'The highest {metric} value is reported for PDB-ID(s): {highest}')\n",
    "\n",
    "    # Plot the distribution, using a Raincloud Plot\n",
    "    f, ax = plt.subplots(figsize=(7, 2))\n",
    "    pt.RainCloud(y = values, data = df, palette = \"Set2\", bw = .2, width_viol = .6, ax = ax, orient = \"h\")\n",
    "    plt.xlabel(f'{metric} {unit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_distribution(df, metric='Resolution', unit='[A]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feebda4",
   "metadata": {},
   "source": [
    "### Binding Site\n",
    "Binding sites were identified manually, using PyMOL. <br>\n",
    "- The **Same_Number**, **Same_Binding_Site** and **Same_Orientation** columns compare all structures and chains with the same ligand. Consequently the boolean value is always the same for a given ligand.\n",
    "    - Same_Number is True, when the same number of ligands is present in each of them\n",
    "    - Same_Binding_Site is True, when \n",
    "    - Same Orientation is True, when  \n",
    "- The **Multisite** column records, if more than one different binding site is occupied by the given ligand, either in the same crystal structure/chain or in different ones. Consequently the boolean value is always the same for a given ligand.\n",
    "- The **Orthosteric** and **Allosteric** columns store, how many ligands of this type are present in the given PDB structure. The **Lig_Order_in_.pdb** records, in which order these are present in the .pdb file. \n",
    "- The **Occurence** column stores, how many crystal structures are present in the PDB, with the given ligand. The **PDB_IDs** column lists all of them for the given ligand, i.e. is always the same for a given ligand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, that the Boolean type column values are always the same for a given ligand!\n",
    "\n",
    "# Specify columns, by which the DataFrame should be grouped\n",
    "group_cols = ['Lig_ID', 'Lig_SMILES', \n",
    "              #'Lig_Name', 'Lig_Formula', 'Lig_MW', 'Lig_CHEMBL_ID', 'Lig_SMILES_Uncharged', \n",
    "              'Multisite', 'Same_Number', 'Same_Binding_Site', 'Same_Orientation']\n",
    "\n",
    "# All columns, not in the group_cols list, should be aggregated to lists\n",
    "def agg_list(x):\n",
    "    return list(x)\n",
    "agg_cols = {col: agg_list for col in df.columns if col not in group_cols}  \n",
    "\n",
    "# Conduct the actual grouping and aggregating\n",
    "binding_site_df = df.groupby(group_cols).agg(agg_cols).reset_index()\n",
    "\n",
    "# Flatten the aggregated columns\n",
    "for col in agg_cols:\n",
    "    # If there is only one value in the list, extract the value, else return the original list\n",
    "    binding_site_df[col] = [(x[0]) if len(x) == 1 and type(x) is list else x for x in binding_site_df[col]]\n",
    "\n",
    "# Add a the molecular structure again\n",
    "PandasTools.AddMoleculeColumnToFrame(df, smilesCol=\"Lig_SMILES\", molCol='Lig_Structure')\n",
    "\n",
    "# Cross-Check, that we did not loose any ligand\n",
    "print(f\"Number of unique Lig-IDs before regrouping data = {df['Lig_ID'].nunique()}\")\n",
    "print(f\"Number of unique Lig-IDs after regrouping data = {binding_site_df['Lig_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b482c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a boolean type column to the dataframe df, which is \n",
    "# True, if the integer or the list of integers's is greater zero, and False otherwise.\n",
    "\n",
    "def add_binding_type_boolean(x):\n",
    "    if isinstance(x, list):\n",
    "        return sum(i for i in x if isinstance(i, (int, float))) > 0\n",
    "    elif isinstance(x, (int, float)):\n",
    "        return x > 0\n",
    "\n",
    "binding_site_df['Orthosteric_Boolean'] = binding_site_df['Orthosteric'].apply(add_binding_type_boolean)\n",
    "binding_site_df['Allosteric_Boolean']  = binding_site_df['Allosteric'].apply(add_binding_type_boolean)\n",
    "\n",
    "binding_site_df.copy().drop(['Lig_Structure'], axis=1).to_excel(\n",
    "    os.path.join(tmp_folder, 'Binding_Site_Analysis.xlsx'), header=True, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Out of {binding_site_df['Lig_ID'].nunique()} ligands in total,\")\n",
    "print(f\"{binding_site_df['Orthosteric_Boolean'].sum()} bind in the ATP (orthosteric) site and\")\n",
    "print(f\"{binding_site_df['Allosteric_Boolean'].sum()} bind at minimum one allosteric site(s).\")\n",
    "\n",
    "print(f\"\\n{len(binding_site_df[binding_site_df['Multisite'] == True])} ligands bind at multiple sites (either at the ATP site plus minimum one allosteric site, or at multiple allosteric sites).\")\n",
    "\n",
    "print(f\"\\nAllosteric Ligands are: {binding_site_df.loc[binding_site_df['Allosteric_Boolean'], 'Lig_ID'].tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
